**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:      |  12   |
| -------------- | --- |
| Student Names: | Shaila Aktar    |
| Student Names: | Muhammad Arij Ashar  |
| Student Names: | Mohammad Sadat Mohtasim   |
| Student Names: | Gaumit Kauts |

# Introduction
In this lab, we present our findings and analysis of a hypothetical system's reliability using two distinct reliability assessment techniques: Reliability Growth Testing and Reliability Demonstration Chart (RDC). We utilized the C-SFRAT and RDC-11 tools to analyze the provided failure data from integration testing and create plots of failure rate and reliability for the system under test (SUT). The objective of this exercise is to gain hands-on experience in assessing the reliability of a system and understanding the advantages and disadvantages of each method. The report outlines our methodology, results, and comparisons between the two techniques, as well as a discussion on the lessons learned during this lab exercise.

# Assessment Using Reliability Growth Testing
Since we faced issues with SRTAT, we chose to go with the C-SFRAT tool for our reliability growth testing. C-SFRAT only accepts .csv files so we converted our failure report from .docx to .csv and also omitted the last column (severity). We also had to change the names of the columns in the file because C-SFRAT wouldn't work otherwise. We report our findings below:

![graph1](https://user-images.githubusercontent.com/90587576/230704929-860fbbea-d41f-406a-834f-0f7330ed06bc.PNG)

Next, we selected a set of models we believed to be a good fit for the data and plotted the graph:

![graph2](https://user-images.githubusercontent.com/90587576/230704987-0c7d9c1f-78ef-4a23-856d-3cfc9df0f8fe.PNG)

Using the Model Comparision tab, we can compare the models using their goodness-of-fit statistics, such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), and choose the best-fitting model or set of models based on these criteria. In this case, since lower values indicate a better fit, we can conclude that the NB2 model best fits our failure data:

![graph3](https://user-images.githubusercontent.com/90587576/230705120-58b3e877-062d-42c6-959f-51c9521afaa7.PNG)

Finally, we generated the Failure Intensity graph:

![graph4](https://user-images.githubusercontent.com/90587576/230705419-d78fa964-5610-4f92-bf49-8a2cbf7ec3c5.PNG)


# Assessment Using Reliability Demonstration Chart 

First of all to determine the MMTFmin we used the formula : MTTF = Total time(in hours)/Number of failures. Referencing failure report-3 we got the following results: total time= 903 seconds  but we chose 975 as the cumulative time bacause we are not sure if the process ended exactly at 903, so 975= 0.271 hours and 24 failures, therefore the MTTFmin is equal to 0.0113.Through the mentioned formula supported by the given data we calculated the MMTF min. Therefore MTTF min is 0.0113 hours/failure or 40.62 seconds/failure.
Following are the graphical representation with respect to our experimentally observed/collected and mathematically calculated MMTFmin(MTTFmin, half of it and twice of it):

<img width="415" alt="Screen Shot 2023-04-07 at 10 47 24 PM" src="https://user-images.githubusercontent.com/104500471/230703651-b252c775-26ee-484a-aa4c-b61f0e2caf96.png">

<img width="597" alt="Screen Shot 2023-04-07 at 10 49 53 PM" src="https://user-images.githubusercontent.com/104500471/230703725-96ef5aae-460c-4f55-b7ec-7f733ea85f29.png">

<img width="412" alt="Screen Shot 2023-04-07 at 10 50 39 PM" src="https://user-images.githubusercontent.com/104500471/230703754-79b91f16-20a0-4307-b738-25a2ae03d494.png">


Advantages of Reliability Demonstration Chart: This tool has several advantages. With the help of this tool we can visualize the data which can further help in interpreting it. Moreover, this tool provide cost-efficiency by providing a strong substratum for testing product's reliability. Furthermore, this tool also helps in providing qualitative measures for the product which is further effective when comes to making decision over product design and maintainance.

Disadvantages of Reliability Demonstration Chart: Although this tool provides a lot of advantages to the users but on the same hand this tool also comes along with some disadvantages. Using this tool can be really time consuming specifically when there are various tests that needs to be performed. Moreover,although this tool is great in testing reliability but this tool does not give a precise number of failures that can potentially take place(edge cases/situations). 

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

Similarities: Both the techniques are used to access the reliability of the product/application. Both the techniques uses various testing situations under predictable outcomes. Both the techniques visualize analyzes through graphical means.Both the techniques uses a pre-defined test scenario to conclude the process to success.Both techniques help the users to identify area of improvements for getting better reliability.

Differences: Both the techniques has numerous differences in their objectives, approach and testing criteria. The main aim of Reliability Growth Testing (RGT) is to detect, report and solve the reliability based issue on an initial development stage whereas the Reliability Demonstration Chart(RDC) helps in keeping a check on whether the reliability requirement meets the pre-defined standard requirements. RGT used various data collections for conducting testing whereas RDC gathers data from product's response concluded during the testing stage. RGT has a straight-up black and white criteria meaning thereby, either it passes the requirement process or it fails it and it depends upon the failure count; whereas the RDC uses a graphical presentation to show the probability of the product to meet its pre-defined reliability based requirements.

# How the team work/effort was divided and managed
The entire team was split into two smaller groups. Each report was tested using each of the various models by one team while the other team tested each report using the various Applicability Analysis techniques. Upon completion of both tests, each team conveyed the whole idea to the other team, and all team members then assessed one another's understanding to ensure that everyone had understood each and every concept. We spoke about how to present the time-between-failure, failure intensity, and reliability graphs, as well as which report to use for trends and what range of meaningful data to utilise. Additionally, we collaborated to talk about the acceptable range of failure rates for the test results.
# 

# Difficulties encountered, challenges overcome, and lessons learned
Setting up the project and choosing the range of usable data using Laplace and other tests was the major hurdle. SRTAT tool was not functional on most of our computers so we had to switch to C-SFRAT. Getting familiar with the tool was also quite a challenge.
# Comments/feedback on the lab itself
This lab was intriguing and difficult at the same time. The straightforward instructions and images made it easier to grasp. That provided us with practical reliability testing experience.
